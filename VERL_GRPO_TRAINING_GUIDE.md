# VeRL GRPO Training Guide for Grounding Tasks

## ðŸš€ Complete Workflow: From Data to Training

### Step 1: Install VeRL

```bash
# Clone VeRL repository
git clone https://github.com/volcengine/verl.git
cd verl

# Install dependencies
pip install -r requirements.txt
pip install -e .

# Optional: Install additional backends
pip install vllm  # for fast inference
pip install sglang  # alternative inference backend
```

### Step 2: Prepare Your Corrected Grounding Dataset

```bash
# Run your corrected dataset creation script
python radvlm/data/create_grounding_verl_corrected.py

# This creates:
# - train.parquet (training data)
# - test.parquet (evaluation data)
```

### Step 3: Create Custom IoU Reward Function

Create `custom_grounding_reward.py`:

```python
import re
import json
import numpy as np
from typing import List, Dict, Any

def extract_bounding_boxes(response: str) -> List[Dict[str, float]]:
    """Extract bounding boxes from model response"""
    # Pattern to match <click>x, y, width, height</click>
    pattern = r'<click>(\d+(?:\.\d+)?),\s*(\d+(?:\.\d+)?),\s*(\d+(?:\.\d+)?),\s*(\d+(?:\.\d+)?)</click>'
    matches = re.findall(pattern, response)
    
    boxes = []
    for match in matches:
        x, y, w, h = map(float, match)
        boxes.append({
            'x': x,
            'y': y, 
            'width': w,
            'height': h
        })
    
    return boxes

def calculate_iou(box1: Dict[str, float], box2: Dict[str, float]) -> float:
    """Calculate IoU between two bounding boxes"""
    # Convert to x1, y1, x2, y2 format
    x1_1, y1_1 = box1['x'], box1['y']
    x2_1, y2_1 = x1_1 + box1['width'], y1_1 + box1['height']
    
    x1_2, y1_2 = box2['x'], box2['y']
    x2_2, y2_2 = x1_2 + box2['width'], y1_2 + box2['height']
    
    # Calculate intersection
    xi1, yi1 = max(x1_1, x1_2), max(y1_1, y1_2)
    xi2, yi2 = min(x2_1, x2_2), min(y2_1, y2_2)
    
    if xi2 <= xi1 or yi2 <= yi1:
        return 0.0
    
    intersection = (xi2 - xi1) * (yi2 - yi1)
    
    # Calculate union
    area1 = (x2_1 - x1_1) * (y2_1 - y1_1)
    area2 = (x2_2 - x1_2) * (y2_2 - y1_2)
    union = area1 + area2 - intersection
    
    return intersection / union if union > 0 else 0.0

def grounding_reward_function(ground_truth: List[Dict[str, float]], 
                            generated_response: str) -> float:
    """Calculate IoU-based reward for grounding tasks"""
    predicted_boxes = extract_bounding_boxes(generated_response)
    
    if not predicted_boxes:
        return 0.0
    
    # Calculate max IoU for each predicted box
    max_iou = 0.0
    for pred_box in predicted_boxes:
        for gt_box in ground_truth:
            iou = calculate_iou(pred_box, gt_box)
            max_iou = max(max_iou, iou)
    
    return max_iou

# Register reward function with VeRL
def get_reward_function():
    return grounding_reward_function
```

### Step 4: Create GRPO Training Configuration

Create `run_grounding_grpo.sh`:

```bash
#!/bin/bash
set -x

# Data paths (adjust to your dataset location)
train_path="$HOME/data/grounding/train.parquet"
test_path="$HOME/data/grounding/test.parquet"

train_files="['$train_path']"
test_files="['$test_path']"

# Run GRPO training
python3 -m verl.trainer.main_ppo \
    algorithm.adv_estimator=grpo \
    data.train_files="$train_files" \
    data.val_files="$test_files" \
    data.train_batch_size=512 \
    data.max_prompt_length=2048 \
    data.max_response_length=1024 \
    data.filter_overlong_prompts=True \
    data.truncation='error' \
    \
    actor_rollout_ref.model.path=Qwen/Qwen2-VL-7B-Instruct \
    actor_rollout_ref.actor.optim.lr=1e-6 \
    actor_rollout_ref.model.use_remove_padding=True \
    actor_rollout_ref.actor.ppo_mini_batch_size=128 \
    actor_rollout_ref.actor.ppo_micro_batch_size_per_gpu=8 \
    \
    actor_rollout_ref.actor.use_kl_loss=True \
    actor_rollout_ref.actor.kl_loss_coef=0.001 \
    actor_rollout_ref.actor.kl_loss_type=low_var_kl \
    actor_rollout_ref.actor.entropy_coeff=0 \
    \
    actor_rollout_ref.model.enable_gradient_checkpointing=True \
    actor_rollout_ref.actor.fsdp_config.param_offload=False \
    actor_rollout_ref.actor.fsdp_config.optimizer_offload=False \
    \
    actor_rollout_ref.rollout.log_prob_micro_batch_size_per_gpu=8 \
    actor_rollout_ref.rollout.tensor_model_parallel_size=1 \
    actor_rollout_ref.rollout.name=vllm \
    actor_rollout_ref.rollout.gpu_memory_utilization=0.6 \
    actor_rollout_ref.rollout.n=4 \
    \
    actor_rollout_ref.ref.log_prob_micro_batch_size_per_gpu=8 \
    actor_rollout_ref.ref.fsdp_config.param_offload=True \
    \
    algorithm.use_kl_in_reward=False \
    trainer.critic_warmup=0 \
    trainer.logger='["console","wandb"]' \
    trainer.project_name='grounding_grpo_training' \
    trainer.experiment_name='qwen2_vl_7b_grounding' \
    trainer.n_gpus_per_node=8 \
    trainer.nnodes=1 \
    trainer.save_freq=10 \
    trainer.test_freq=5 \
    trainer.total_epochs=20 \
    \
    reward_model.enable=True \
    reward_model.path=custom_grounding_reward.py \
    $@
```

### Step 5: Key GRPO Configuration Parameters

```yaml
# Critical GRPO settings
algorithm.adv_estimator: grpo  # Use GRPO instead of GAE
actor_rollout_ref.rollout.n: 4  # Generate 4 completions per prompt (group sampling)
actor_rollout_ref.actor.use_kl_loss: True  # Enable KL regularization
actor_rollout_ref.actor.kl_loss_coef: 0.001  # KL loss coefficient
algorithm.use_kl_in_reward: False  # Don't add KL to reward (handled in loss)

# Performance settings
data.train_batch_size: 512  # Global batch size for prompts
actor_rollout_ref.actor.ppo_mini_batch_size: 128  # Mini-batch size for updates
actor_rollout_ref.actor.ppo_micro_batch_size_per_gpu: 8  # Per-GPU micro-batch size
```

### Step 6: Run GRPO Training

```bash
# Make script executable
chmod +x run_grounding_grpo.sh

# Run training
./run_grounding_grpo.sh

# Optional: Run with specific GPU configuration
./run_grounding_grpo.sh trainer.n_gpus_per_node=4 trainer.nnodes=2
```

### Step 7: Monitor Training

```bash
# Check training progress
tail -f logs/grounding_grpo_training.log

# Monitor with W&B (if configured)
# Check your W&B dashboard for metrics
```

### Step 8: Advanced Configuration Options

For better performance, you can use these advanced settings:

```bash
# With Megatron backend for larger models
python3 -m verl.trainer.main_ppo \
    # ... other configs ...
    actor_rollout_ref.model.backend=megatron \
    actor_rollout_ref.model.megatron.tensor_model_parallel_size=2 \
    actor_rollout_ref.model.megatron.pipeline_model_parallel_size=2

# With sequence balancing for efficiency
python3 -m verl.trainer.main_ppo \
    # ... other configs ...
    data.enable_sequence_balance=True \
    data.sequence_balance.method=pack

# With LoRA for memory efficiency
python3 -m verl.trainer.main_ppo \
    # ... other configs ...
    actor_rollout_ref.model.peft.enable=True \
    actor_rollout_ref.model.peft.peft_type=lora \
    actor_rollout_ref.model.peft.lora_r=64 \
    actor_rollout_ref.model.peft.lora_alpha=16
```

### Step 9: Evaluation

After training, evaluate your model:

```bash
# Use RadVLM evaluation infrastructure
python radvlm/evaluation/evaluate_grounding.py \
    --model_path ./checkpoints/grounding_grpo_training/epoch_15 \
    --test_data ./data/grounding/test.parquet \
    --output_dir ./evaluation_results
```

## ðŸŽ¯ Key Differences for Grounding Tasks

### 1. **Data Format**: 
- âœ… Your corrected format matches VeRL specs exactly
- âœ… Only prompts in training data (no answers)
- âœ… IoU reward calculation from ground truth

### 2. **Reward Function**:
- âœ… Custom IoU-based reward function
- âœ… Handles multiple bounding boxes
- âœ… Returns 0.0 for invalid predictions

### 3. **Model Configuration**:
- âœ… Vision-language model (Qwen2-VL)
- âœ… Longer sequence lengths for image descriptions
- âœ… Group sampling (n=4) for GRPO

## ðŸ”§ Troubleshooting

### Common Issues:

1. **OOM Errors**: Reduce batch sizes or enable gradient checkpointing
2. **Slow Training**: Use vLLM backend and sequence balancing
3. **Poor Rewards**: Check IoU calculation and box extraction logic
4. **Model Divergence**: Lower learning rate or increase KL coefficient

### GPU Memory Optimization:

```bash
# For limited GPU memory
actor_rollout_ref.actor.ppo_micro_batch_size_per_gpu=4
actor_rollout_ref.model.enable_gradient_checkpointing=True
actor_rollout_ref.actor.fsdp_config.param_offload=True
actor_rollout_ref.rollout.gpu_memory_utilization=0.5
```

## ðŸŽ‰ Success Metrics

Monitor these metrics during training:

- **Reward/train**: Should increase over time
- **Reward/val**: Should improve on validation set
- **KL Divergence**: Should stay bounded (< 1.0)
- **Loss/actor**: Should decrease
- **IoU Average**: Custom metric for grounding accuracy

Your corrected implementation is 100% compatible with VeRL GRPO training! The key insight is that GRPO generates multiple completions during training and learns from IoU rewards, which is exactly what your format supports.