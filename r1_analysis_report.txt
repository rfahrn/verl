================================================================================
R1 REWARD FUNCTION - COMPREHENSIVE ANALYSIS REPORT
================================================================================

MATHEMATICAL FORMULATION
----------------------------------------
LaTeX equation for thesis:

\begin{equation}
\label{eq:r1_reward}
\mathcal{R}_1(\mathcal{P}, \mathcal{G}) = 
\begin{cases}
\rho_{\text{CN}} & \text{if } |\mathcal{P}| = 0 \text{ and } |\mathcal{G}| = 0 \quad \text{(Correct Negative)} \\[0.3em]
0 & \text{if } (|\mathcal{P}| > 0 \text{ and } |\mathcal{G}| = 0) \text{ or } (|\mathcal{P}| = 0 \text{ and } |\mathcal{G}| > 0) \\[0.3em]
\text{mAP}_{\tau}(\mathcal{P}, \mathcal{G}) & \text{otherwise}
\end{cases}
\end{equation}

Where:
- P: Set of predicted bounding boxes
- G: Set of ground truth bounding boxes
- ρ_CN = 0.2: Correct negative bonus
- τ = 0.5: IoU threshold
- mAP: Mean Average Precision at IoU threshold τ

SCENARIO ANALYSIS SUMMARY
----------------------------------------

LaTeX table for thesis:

\begin{table}[h]
\centering
\caption{R1 Reward Function Behavior Across Different Scenarios}
\label{tab:r1_scenarios}
\begin{tabular}{l|c|c|c|c|c|c}
\hline
Scenario & $|\mathcal{P}|$ & $|\mathcal{G}|$ & TP & FP & FN & Reward \\
\hline\hline
Correct negative prediction (n & 0 & 0 & 0 & 0 & 0 & 0.200 \\
Single hallucinated box (FP) & 1 & 0 & 0 & 1 & 0 & 0.000 \\
Single missed detection (FN) & 0 & 1 & 0 & 0 & 1 & 0.000 \\
Perfect single box match & 1 & 1 & 1 & 0 & 0 & 1.000 \\
Good overlap (IoU > 0.5) & 1 & 1 & 1 & 0 & 0 & 1.000 \\
Poor overlap (IoU < 0.5) & 1 & 1 & 0 & 1 & 1 & 0.000 \\
1 prediction, 2 GT boxes & 1 & 2 & 1 & 0 & 1 & 0.500 \\
3 predictions, 3 GT, 2 matches & 3 & 3 & 2 & 1 & 1 & 0.667 \\
10 GT boxes, all perfectly mat & 10 & 10 & 10 & 0 & 0 & 1.000 \\
\hline
\end{tabular}
\end{table}

REWARD FUNCTION PROPERTIES
----------------------------------------

1. CONTINUITY:
   - The reward function exhibits discontinuity at IoU = τ (0.5)
   - Smooth transitions within matching and non-matching regions
   - Gradient is well-behaved except at the threshold boundary

2. RANGE AND DISTRIBUTION:
   - Reward range: [0.0, 1.0]
   - Mean reward across scenarios: 0.556
   - Std deviation: 0.432

3. EDGE CASE BEHAVIOR:
   - true_negative:
     Mean reward: 0.200, Std: 0.000
   - hallucination:
     Mean reward: 0.000, Std: 0.000
   - missed_detection:
     Mean reward: 0.000, Std: 0.000
   - one_to_one:
     Mean reward: 0.667, Std: 0.471
   - one_to_many:
     Mean reward: 0.350, Std: 0.150
   - many_to_one:
     Mean reward: 1.000, Std: 0.000
   - many_to_many:
     Mean reward: 0.853, Std: 0.197

4. LEARNING SIGNAL QUALITY:
   - Clear differentiation between correct/incorrect predictions
   - Proportional rewards for partial matches
   - Handles multi-object scenarios appropriately
   - Small positive reward for correct negatives encourages learning

5. SCALABILITY:
   - Consistent behavior from 0 to 10+ boxes
   - Computational complexity: O(n*m) for n predictions, m ground truth
   - Memory efficient for typical object detection scenarios

CRITICAL OBSERVATIONS
----------------------------------------

STRENGTHS:
1. Simple and interpretable formulation
2. Aligns with standard object detection metrics (mAP@0.5)
3. Handles edge cases explicitly
4. Provides non-zero reward for correct negatives
5. Computationally efficient

LIMITATIONS:
1. Hard threshold at IoU = 0.5 creates discontinuity
2. No partial credit for IoU < 0.5
3. Equal weight for all boxes (no importance weighting)
4. Greedy matching may not find optimal assignment

RECOMMENDATIONS FOR IMPROVEMENT:
1. Consider soft IoU thresholding for smoother gradients
2. Add confidence weighting for predicted boxes
3. Implement Hungarian algorithm for optimal matching
4. Consider class-specific reward weights
